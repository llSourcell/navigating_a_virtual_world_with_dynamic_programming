{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Demo - We'll build an AI that can navigate from point A to point B on a Frozen Lake without falling into a hole! \n",
    "\n",
    "![alt text](https://i.ytimg.com/vi/xgoO54qN4lY/maxresdefault.jpg \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "# Framing our Problem: The Markov Decision Process\n",
    "\n",
    "\n",
    "\n",
    "- In RL we have an agent interacting with an environment\n",
    "- At each time step, the agent performs an action which leads to two things: changing the environment state and the agent (possibly) recieving a reward (or penalty) from the enviroment. \n",
    "- The goal of the agent is to discover an optimal policy (i.e. what actions to do in each state) such that it maximizes the total value of rewards recieved from the environment in response to its actions. \n",
    "- MDPis used to describe the agent/ environemnt interaction settings in a formal way.\n",
    "\n",
    "MDP consists of a tuple of 5 elements:\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ml-sep-09-091009141615-phpapp01/95/regretbased-reward-elicitation-for-markov-decision-processes-39-728.jpg?cb=1255098159  \"Logo Title Text 1\")\n",
    "\n",
    "- S : Set of states. At each time step the state of the environment is an element s ‚àà S.\n",
    "- A: Set of actions. At each time step the agent choses an action a ‚àà A to perform.\n",
    "- State transition model that describes how the environment state changes when the user performs an action a depending on the action aand the current state st.\n",
    "- Reward model that describes the real-valued reward value that the agent recieves from the environment after performing an action. In MDP the the reward value depends on the current state and the action performed.\n",
    "- discount factor that controls the importance of future rewards.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*nS-MKI7pY8PeQLDCQ64xCA@2x.png  \"Logo Title Text 1\")\n",
    "\n",
    "- The way by which the agent choses which action to perform is named the agent policy \n",
    "- Policy is a function that takes the current environment state to return an action. \n",
    "- The policy is often denoted by the symbol ùõë\n",
    "\n",
    "Let‚Äôs now differentiate between two types environments.\n",
    "\n",
    "![alt text](http://slideplayer.com/slide/6241983/21/images/17/Deterministic+vs+stochastic+environments.jpg  \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "- Deterministic environments are easier to solve, because the agent knows how to plan its actions with no-uncertaintiy given the environment MDP. \n",
    "- The goal of the agent is to pick the best policy that will maximize the total rewards recieved from the environment.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*Jix2ScBmffb1e5MpZCiwMg@2x.png  \"Logo Title Text 1\")\n",
    "\n",
    "- where T is the horizon (episode length) which can be infinity if there is maximum length for the episode\n",
    "- The reason for using discount factor is to prevent the total reward from going to infinity (because 0 ‚â§ ùõæ ‚â§ 1)\n",
    "- it also models the agent behaviour when the agent prefers immediate rewards than rewards that are potentially recieved far away in the future\n",
    "\n",
    "### The solution to an MDP is called a \"policy\" and it simply specifies the best action to take for each of the states.\n",
    "\n",
    "\n",
    "### But Although the policy is what we are after, we will actually compute a value function because we can easily derive the policy if we have the value function. A value function is similar to a policy, except that instead of specifying an action for each state, it specifies a numerical value for each state.\n",
    "\n",
    "- Two fundamental methods for solving MDPs are policy-iteration and value-iteration algorithms\n",
    "- Both assume that the agent knows the MDP model of the world (model-based)\n",
    "- Later on we'll discuss Q-Learning (model-based)\n",
    "\n",
    "![alt text](http://images.slideplayer.com/32/10082567/slides/slide_16.jpg  \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "![alt text](http://player.slideplayer.com/8/2403707/data/images/img25.jpg  \"Logo Title Text 1\")\n",
    "\n",
    "- Value iteration computes the optimal state value function by iteratively improving the estimate of V(s)\n",
    "- The algorithm initialize V(s) to arbitrary random values. \n",
    "- It repeatedly updates the Q(s, a) and V(s) values until they convergs. \n",
    "- Value iteration is guranteed to converge to the optimal values.\n",
    "\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "![alt text](http://player.slideplayer.com/8/2403707/data/images/img23.jpg  \"Logo Title Text 1\")\n",
    "\n",
    "- policy-iteration instead of repeated improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. \n",
    "- Policy iteration is also guranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm.\n",
    "\n",
    "\n",
    "# In Policy Iteration algorithms, you start with a random policy, then find the value function of that policy (policy evaluation step), then find an new (improved) policy based on the previous value function, and so on. In this process, each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). \n",
    "\n",
    "# In Value Iteration, you start with a random value function and then find a new (improved) value function in a iterative process, until reaching the optimal value function. Notice that you can derive easily the optimal policy from the optimal value function. \n",
    "\n",
    "\n",
    "## Policy iteration is generally faster than value iteration as policy converges more quickly than value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
